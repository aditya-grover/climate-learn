{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/\"\n",
    "new_save_dir = \"/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test_new/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "file_list = glob.glob(os.path.join(path, \"*.npz\"))\n",
    "file_list = [f for f in file_list if \"climatology\" not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2017, 2019))\n",
    "file_list_by_years = [[] for _ in years]\n",
    "for file_name in file_list:\n",
    "    year = int((file_name.split(\"/\")[-1]).split(\"_\")[0])\n",
    "    year_index = year - years[0]\n",
    "    file_list_by_years[year_index].append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_func(file_name):\n",
    "    index = int(((file_name.split(\"/\")[-1]).split(\"_\")[1]).split(\".\")[0])\n",
    "    return index\n",
    "for file_list_by_year in file_list_by_years:\n",
    "    file_list_by_year.sort(key = sort_func, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_0.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_1.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_2.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_3.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_4.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_5.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_6.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_7.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_8.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_9.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_10.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_11.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_12.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_13.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_14.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2017_15.npz'],\n",
       " ['/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_0.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_1.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_2.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_3.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_4.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_5.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_6.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_7.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_8.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_9.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_10.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_11.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_12.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_13.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_14.npz',\n",
       "  '/data0/datasets/weatherbench/data/weatherbench/era5/5.625deg_npz/test/2018_15.npz']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list_by_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_threshold = torch.load(\"./low_threshold.pt\").numpy()\n",
    "high_threshold = torch.load(\"./high_threshold.pt\").numpy()\n",
    "low_threshold = np.squeeze(low_threshold, axis=0)\n",
    "high_threshold = np.squeeze(high_threshold, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_horizon = 7 * 24\n",
    "for file_list in file_list_by_years:\n",
    "    yearly_data = {}\n",
    "    n_instances_in_shard = 0\n",
    "    for file in file_list:\n",
    "        data = np.load(file)\n",
    "        if yearly_data == {}:\n",
    "            yearly_data = data\n",
    "            random_key = next(iter(data.keys()))\n",
    "            n_instances_in_shard = data[random_key].shape[0]\n",
    "        else:\n",
    "            yearly_data = {k: np.concatenate((yearly_data[k], data[k]), axis=0) for k in yearly_data.keys()}\n",
    "            random_key = next(iter(data.keys()))\n",
    "            assert n_instances_in_shard == data[random_key].shape[0]\n",
    "    air_temp = yearly_data[\"2m_temperature\"]\n",
    "    mean_tensor = []\n",
    "    for i in range(time_horizon, air_temp.shape[0]):\n",
    "        curr_mean = np.mean(air_temp[i-time_horizon: i], axis=0)\n",
    "        mean_tensor.append(curr_mean)\n",
    "    mean_tensor = np.stack(mean_tensor, axis=0)\n",
    "\n",
    "    l_mean_tensor = np.roll(mean_tensor, 1, -1)\n",
    "    r_mean_tensor = np.roll(mean_tensor, -1, -1)\n",
    "    d_mean_tensor = np.roll(mean_tensor, 1, -2)\n",
    "    u_mean_tensor = np.roll(mean_tensor, -1, -2)\n",
    "\n",
    "    ld_mean_tensor = np.roll(l_mean_tensor, 1, -2)\n",
    "    lu_mean_tensor = np.roll(l_mean_tensor, -1, -2)\n",
    "    rd_mean_tensor = np.roll(r_mean_tensor, 1, -2)\n",
    "    ru_mean_tensor = np.roll(r_mean_tensor, -1, -2)\n",
    "\n",
    "    g_mean_tensor = 4 * mean_tensor\n",
    "    g_mean_tensor += l_mean_tensor + r_mean_tensor + d_mean_tensor + u_mean_tensor\n",
    "    g_mean_tensor += 0.25 * (ld_mean_tensor + lu_mean_tensor + rd_mean_tensor + ru_mean_tensor)\n",
    "    g_mean_tensor = g_mean_tensor / 9\n",
    "\n",
    "    threshold_instances = np.zeros_like(air_temp[0], dtype = air_temp.dtype)\n",
    "    air_temp_extreme_mask = np.zeros_like(air_temp, dtype = air_temp.dtype)\n",
    "    for i in range(time_horizon, air_temp.shape[0]):\n",
    "        curr_g_mean = g_mean_tensor[i - time_horizon]\n",
    "        curr_mask = np.logical_or(curr_g_mean < low_threshold, curr_g_mean > high_threshold).astype(air_temp.dtype)\n",
    "        air_temp_extreme_mask[i] = curr_mask\n",
    "        threshold_instances += curr_mask\n",
    "    n_instances = np.min(threshold_instances)\n",
    "    yearly_data[\"2m_temperature_extreme_mask\"] = air_temp_extreme_mask\n",
    "    \n",
    "    for shard_id, file in enumerate(file_list):\n",
    "        start_index = shard_id * n_instances_in_shard\n",
    "        end_index = start_index + n_instances_in_shard\n",
    "        new_file_name = os.path.join(new_save_dir, file.split(\"/\")[-1])\n",
    "        sharded_data = {k: yearly_data[k][start_index:end_index] for k in yearly_data.keys()}\n",
    "        np.savez(new_file_name, **sharded_data)\n",
    "\n",
    "    print(air_temp_extreme_mask.sum(), air_temp.shape[0]*air_temp.shape[-1]*air_temp.shape[-2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1856725.0/17891328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfile_list = glob.glob(os.path.join(new_save_dir, \"*.npz\"))\n",
    "newfile_list = [f for f in newfile_list if \"climatology\" not in f]\n",
    "years = list(range(2017, 2019))\n",
    "newfile_list_by_years = [[] for _ in years]\n",
    "for file_name in newfile_list:\n",
    "    year = int((file_name.split(\"/\")[-1]).split(\"_\")[0])\n",
    "    year_index = year - years[0]\n",
    "    newfile_list_by_years[year_index].append(file_name)\n",
    "for newfile_list_by_year in newfile_list_by_years:\n",
    "    newfile_list_by_year.sort(key = sort_func, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for newfile_list, file_list in zip(newfile_list_by_years, file_list_by_years):\n",
    "    for new_file, file in zip(newfile_list, file_list):\n",
    "        new_data = np.load(new_file)\n",
    "        data = np.load(file)\n",
    "        for k in new_data.keys():\n",
    "            if k == \"2m_temperature_extreme_mask\":\n",
    "                continue\n",
    "            else:\n",
    "                assert (new_data[k] == data[k]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_data[\"2m_temperature\"] == data[\"2m_temperature\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[\"2m_temperature_extreme_mask\"][0][0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

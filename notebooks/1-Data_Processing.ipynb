{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99jkSa_KmrDH"
   },
   "source": [
    "# Data Processing\n",
    "\n",
    "ClimateLearn makes it super easy to prepare data for your machine learning pipelines. In this tutorial, we'll see how to download [ERA5](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5) data from [WeatherBench](https://github.com/pangeo-data/WeatherBench) and prepare it for both the forecasting and [downscaling](https://uaf-snap.org/how-do-we-do-it/downscaling) tasks. This tutorial is intended for use in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab setup\n",
    "You might need to restart the kernel after installing ClimateLearn so that your Colab environment knows to use the correct package versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install climate-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "\n",
    "The following cell will take several minutes to run - the scale of climate data is huge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmQG73ZpQNHP",
    "outputId": "e4d79d00-c5b8-4bb3-bf2a-75f94e737bec"
   },
   "outputs": [],
   "source": [
    "from climate_learn.data import download\n",
    "\n",
    "root = \"/content/drive/MyDrive/ClimateLearn\"\n",
    "source = \"weatherbench\"\n",
    "dataset = \"era5\"\n",
    "resolution = \"5.625\"\n",
    "variable = \"2m_temperature\"\n",
    "\n",
    "download(root=root, source=source, dataset=dataset, resolution=resolution, variable=variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSt6h_Q-oqjK"
   },
   "source": [
    "ClimateLearn comes with some utilities to view the downloaded data in its raw format. This can be useful as a quick sanity check that you have the data you expect. Climate data is natively stored in the [NetCDF format](https://www.unidata.ucar.edu/software/netcdf/), which means it comes bundled with lots of helpful named metadata such as latitude, longitude, and time. However, we want the data in a form that can be easily ingested by PyTorch machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "97hHL2Z7-Z86",
    "outputId": "2b960774-065b-4eb4-d3e3-e18001acab32"
   },
   "outputs": [],
   "source": [
    "from climate_learn.utils.data import load_dataset, view\n",
    "\n",
    "my_dataset = load_dataset(f\"{root}/data/{source}/{dataset}/{resolution}/{variable}\")\n",
    "view(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XM3rITW9Y3-"
   },
   "source": [
    "## Preparing data for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we specify the dataset arguments. The temporal range of ERA5 data on WeatherBench is 1979 to 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EK2UD49hQ3om",
    "outputId": "6650bc42-8a22-4f53-fa4e-b32b8c5f6887"
   },
   "outputs": [],
   "source": [
    "from climate_learn.data.climate_dataset.args import ERA5Args\n",
    "\n",
    "years = range(1979, 2018)\n",
    "data_args = ERA5Args(\n",
    "    root_dir=f\"{root}/data/{source}/{dataset}/{resolution}/\",\n",
    "    variables=[variable],\n",
    "    years=years\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify the task arguments. In this case we are interested in forecasting only `2m_temperature` using only `2m_temperature`, but one could specify additional variables, provided that the data for those variables is downloaded. The prediction range is in hours, so if we want to predict 3 days ahead, we provide `3*24`. Further, we subsample every 6 hours of the day since weather conditions do not change significantly on hourly intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.data.task.args import ForecastingArgs\n",
    "\n",
    "forecasting_args = ForecastingArgs(\n",
    "    in_vars=[variable],\n",
    "    out_vars=[variable],\n",
    "    pred_range=3*24,\n",
    "    subsample=6\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the scale of climate data is huge, we need to specify how we want to load the data in the CPU memory. ClimateLearn allows us to either load the entire data into memory or shard it and then load it in chunks. The latter comes with the overhead of loading data multiple times in every epoch. In this tutorial, as the data has just single variable, we would use the first technique to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.data.dataset.args import MapDatasetArgs\n",
    "\n",
    "map_dataset_args = MapDatasetArgs(\n",
    "    climate_dataset_args=data_args,\n",
    "    task_args=forecasting_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we specify the data module, where we define our train-validation-testing split and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.data import DataModuleArgs, DataModule\n",
    "\n",
    "data_module_args = DataModuleArgs(\n",
    "    dataset_args=map_dataset_args,\n",
    "    train_start_year=1979,\n",
    "    val_start_year=2015,\n",
    "    test_start_year=2017,\n",
    "    end_year=2018\n",
    ")\n",
    "\n",
    "data_module = DataModule(\n",
    "    data_module_args=data_module_args,\n",
    "    batch_size=128,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srfsF01OLV-C"
   },
   "source": [
    "## Preparing data for downscaling\n",
    "\n",
    "In the [downscaling task](https://uaf-snap.org/how-do-we-do-it/downscaling), we want to build a machine learning model that can map low-resolution weather patterns (source) to high-resolution weather patterns (target). In the previous section, we already downloaded a dataset for `2m_temperature` at 5.625 degrees resolution. Here, let's download a dataset also for `2m_temperature` but at 2.8125 degrees resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3tRve6-h0sI",
    "outputId": "bd5501e2-ac7c-4b2f-9edc-584c3e054e74"
   },
   "outputs": [],
   "source": [
    "hi_resolution = \"2.8125\"\n",
    "download(root=root, source=source, dataset=dataset, resolution=hi_resolution, variable=variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsR8lhdjXejR"
   },
   "source": [
    "Next, we specify the dataset arguments. This is the same procedure as for forecasting, but with two datasets now: one set of arguments is for the source, and another set of arguments is for the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T9N7cL4oFKm",
    "outputId": "897e622e-6f29-4211-be36-c2e8250c4bf3"
   },
   "outputs": [],
   "source": [
    "lowres_data_args = ERA5Args(\n",
    "    root_dir=f\"{root}/data/{source}/{dataset}/{resolution}/\",\n",
    "    variables=[variable],\n",
    "    years=years\n",
    ")\n",
    "\n",
    "highres_data_args = ERA5Args(\n",
    "    root_dir=f\"{root}/data/{source}/{dataset}/{hi_resolution}\",\n",
    "    variables=[variable],\n",
    "    years=years\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to wrap these multiple dataset sources into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.data.climate_dataset.args import StackedClimateDatasetArgs\n",
    "\n",
    "data_args = StackedClimateDatasetArgs(\n",
    "    data_args=[lowres_data_args, highres_data_args]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we specify the task arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.data.task.args import DownscalingArgs\n",
    "\n",
    "downscaling_args = DownscalingArgs(\n",
    "    in_vars=[variable],\n",
    "    out_vars=[variable],\n",
    "    subsample=6,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again need to specifiy how to load data into memory. This time let's try the sharding approach. Other than the `climate_dataset_args` and `task_args`, we also need to specify the number of chunks we want to shard the dataset into. Note that in `ERA5` the data for different years is stored in different files. Thus, we can't have nuber of chunks greater than the number of training years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.data.dataset.args import ShardDatasetArgs\n",
    "\n",
    "shard_data_args = ShardDatasetArgs(\n",
    "    climate_dataset_args=data_args,\n",
    "    task_args=downscaling_args,\n",
    "    n_chunks=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we specify the data module, which looks the same as for the forecasting task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module_args = DataModuleArgs(\n",
    "    dataset_args=shard_data_args,\n",
    "    train_start_year=1979,\n",
    "    val_start_year=2015,\n",
    "    test_start_year=2017,\n",
    "    end_year=2018\n",
    ")\n",
    "\n",
    "data_module = DataModule(\n",
    "    data_module_args=data_module_args,\n",
    "    batch_size=128,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congralutions! Now you know how to load and process data with ClimateLearn. Please visit our [docs](https://climatelearn.readthedocs.io/en/latest/user-guide/datasets.html) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b35d5811d64db97cad819926e9e0ba09b354a75e2ee95b259c11201fc783944"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

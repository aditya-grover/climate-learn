{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99jkSa_KmrDH"
   },
   "source": [
    "# Data Processing\n",
    "\n",
    "ClimateLearn makes it super easy to prepare data for your machine learning pipelines. In this tutorial, we'll see how to download [ERA5](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5) data from [WeatherBench](https://github.com/pangeo-data/WeatherBench) and prepare it for both the forecasting and [downscaling](https://uaf-snap.org/how-do-we-do-it/downscaling) tasks. This tutorial is intended for use in Google Colab.\n",
    "\n",
    "See the [ClimateLearn docs](https://climatelearn.readthedocs.io/en/latest/user-guide/datasets.html) for instructions on working with other datasets, such as [CMIP6](https://www.carbonbrief.org/cmip6-the-next-generation-of-climate-models-explained/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install climate-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n",
    "\n",
    "The following cell will take several minutes to run - the scale of climate data is huge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmQG73ZpQNHP",
    "outputId": "e4d79d00-c5b8-4bb3-bf2a-75f94e737bec"
   },
   "outputs": [],
   "source": [
    "from climate_learn.data import download\n",
    "\n",
    "root = \"/content/drive/MyDrive/ClimateLearn\"\n",
    "source = \"weatherbench\"\n",
    "dataset = \"era5\"\n",
    "resolution = \"5.625\"\n",
    "variable = \"2m_temperature\"\n",
    "\n",
    "download(root=root, source=source, dataset=dataset, resolution=resolution, variable=variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSt6h_Q-oqjK"
   },
   "source": [
    "ClimateLearn comes with some utilities to view the downloaded data in its raw format. This can be useful as a quick sanity check that you have the data you expect. Climate data is natively stored in the [NetCDF format](https://www.unidata.ucar.edu/software/netcdf/), which means it comes bundled with lots of helpful named metadata such as latitude, longitude, and time. However, we want the data in a form that can be easily ingested by PyTorch machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "97hHL2Z7-Z86",
    "outputId": "2b960774-065b-4eb4-d3e3-e18001acab32"
   },
   "outputs": [],
   "source": [
    "from climate_learn.utils.data import load_dataset, view\n",
    "\n",
    "dataset = load_dataset(f\"{root}/data/{source}/{dataset}/{resolution}/{varibale}\")\n",
    "view(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XM3rITW9Y3-"
   },
   "source": [
    "## Preparing data for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we specify the dataset arguments. The temporal range of ERA5 data on WeatherBench is 1979 to 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EK2UD49hQ3om",
    "outputId": "6650bc42-8a22-4f53-fa4e-b32b8c5f6887"
   },
   "outputs": [],
   "source": [
    "from climate_learn.data.climate_dataset.args import ERA5Args\n",
    "\n",
    "years = range(1979, 2018)\n",
    "data_args = ERA5Args(\n",
    "    root_dir=f\"{root}/data/{source}/{dataset}/{resolution}/\",\n",
    "    variables=[variable],\n",
    "    years=years\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify the task arguments. In this case we are interested in forecasting only `2m_temperature` using only `2m_temperature`, but one could specify additional variables, provided that the data for those variables is downloaded. The prediction range is in hours, so if we want to predict 3 days ahead, we provide `3*24`. Further, we subsample every 6 hours of the day since weather conditions do not change significantly on hourly intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.data.tasks.args import ForecastingArgs\n",
    "\n",
    "forecasting_args = ForecastingArgs(\n",
    "    dataset_args=data_args,\n",
    "    in_vars=[variable],\n",
    "    out_vars=[variable],\n",
    "    pred_range=3*24,\n",
    "    subsample=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we specify the data module, where we define our train-validation-testing split and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.data import DataModuleArgs, DataModule\n",
    "\n",
    "data_module_args = DataModuleArgs(\n",
    "    task_args=forecasting_args,\n",
    "    train_start_year=1979,\n",
    "    val_start_year=2015,\n",
    "    test_start_year=2017,\n",
    "    end_year=2018\n",
    ")\n",
    "\n",
    "data_module = DataModule(\n",
    "    data_module_args=data_module_args,\n",
    "    batch_size=128,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srfsF01OLV-C"
   },
   "source": [
    "## Preparing data for downscaling\n",
    "\n",
    "In the [downscaling task](https://uaf-snap.org/how-do-we-do-it/downscaling), we want to build a machine learning model that can map low-resolution weather patterns (source) to high-resolution weather patterns (target). In the previous section, we already downloaded a dataset for `2m_temperature` at 5.625 degrees resolution. Here, let's download a dataset also for `2m_temperature` but at 2.8125 degrees resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3tRve6-h0sI",
    "outputId": "bd5501e2-ac7c-4b2f-9edc-584c3e054e74"
   },
   "outputs": [],
   "source": [
    "hi_resolution = \"2.8125\"\n",
    "download(root=root, source=source, dataset=dataset, resolution=hi_resolution, variable=variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsR8lhdjXejR"
   },
   "source": [
    "Next, we specify the dataset arguments. This is the same procedure as for forecasting, but with two datasets now: one set of arguments is for the source, and another set of arguments is for the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T9N7cL4oFKm",
    "outputId": "897e622e-6f29-4211-be36-c2e8250c4bf3"
   },
   "outputs": [],
   "source": [
    "from climate_learn.data.climate_dataset.args import ERA5Args\n",
    "\n",
    "lowres_data_args = ERA5Args(\n",
    "    root_dir=f\"{root}/data/{source}/{dataset}/{resolution}/\",\n",
    "    variables=[variable],\n",
    "    years=years\n",
    ")\n",
    "\n",
    "highres_data_args = ERA5Args(\n",
    "    root_dir=f\"{root}/data/{source}/{dataset}/{hi_resolution}\",\n",
    "    variables=[variable],\n",
    "    years=years\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we specify the task arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.data.tasks.args import DownscalingArgs\n",
    "\n",
    "downscaling_args = DownscalingArgs(\n",
    "    dataset_args=data_args,\n",
    "    highres_dataset_args=highres_data_args,\n",
    "    in_vars=[variable],\n",
    "    out_vars=[variable],\n",
    "    subsample=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we specify the data module, which looks the same as for the forecasting task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate_learn.data import DataModuleArgs, DataModule\n",
    "\n",
    "data_module_args = DataModuleArgs(\n",
    "    task_args=downscaling_args,\n",
    "    train_start_year=1979,\n",
    "    val_start_year=2015,\n",
    "    test_start_year=2017,\n",
    "    end_year=2018\n",
    ")\n",
    "\n",
    "data_module = DataModule(\n",
    "    data_module_args=data_module_args,\n",
    "    batch_size=128,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congralutions! Now you know how to load and process data with ClimateLearn. Please visit our [docs](https://climatelearn.readthedocs.io/en/latest/user-guide/datasets.html) to learn more."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b35d5811d64db97cad819926e9e0ba09b354a75e2ee95b259c11201fc783944"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

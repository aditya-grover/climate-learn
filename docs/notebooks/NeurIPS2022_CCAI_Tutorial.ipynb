{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13i7KQ9t-CV8"
   },
   "source": [
    "## **Machine Learning for Predicting Climate Extremes**\n",
    "\n",
    "\n",
    "Author(s):\n",
    "*   Hritik Bansal, University of California Los Angeles, hbansal@g.ucla.edu\n",
    "*   Shashank Goel, University of California Los Angeles, shashankgoel@g.ucla.edu\n",
    "*   Tung Nguyen,   University of California Los Angeles, tungnd@g.ucla.edu\n",
    "*   Aditya Grover, University of California Los Angeles, adityag@cs.ucla.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNv0ANr5WcD_"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "*   [Target Audience](#target-audience)\n",
    "*   [Overview](#overview)\n",
    "*   [Contributions](#contributions)\n",
    "*   [Software Requirements](software-requirements)\n",
    "*   [Data Source](data-source)\n",
    "*   [Temporal Forecasting](temporal-forecasting)\n",
    "*   [Spatial Downscaling](spatial-downscaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5sbM_JPpdMR"
   },
   "source": [
    "<a name=\"target-audience\"></a>\n",
    "# Target Audience\n",
    "\n",
    "\n",
    "*   Climate scientists seeking to explore ML tools, technologies, and resources to tackle a domain-specific problem in climate change. \n",
    "*   Data scientists with prior background in deep learning looking for concrete examples on how to model climate extremes using AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH81wjfsJsv1"
   },
   "source": [
    "<a name=\"overview\"></a>\n",
    "# Overview\n",
    "\n",
    "Climate change has led to a rapid increase in the occurrence of extreme weather events globally, including floods, droughts, and wildfires. In the longer term, some regions will experience aridification while others will risk sinking due to rising sea levels. Typically, such predictions are done via weather and climate models that simulate the physical interactions between the atmospheric, oceanic, and land surface processes that operate at different scales. Due to the inherent complexity, these climate models can be inaccurate or computationally expensive to run, especially for detecting climate extremes at high spatiotemporal resolutions. In this tutorial, we aim to introduce the participants to machine learning approaches for addressing two fundamental challenges:\n",
    "\n",
    "(1) **Temporal Forecasting**: The goal is to predict climate variables into the future. \\\n",
    "(2) **Spatial Downscaling**: The goal is to learn a mapping that transforms low-resolution outputs of climate models into high-resolution regional forecasts. \n",
    "\n",
    "We illustrate the two key challenges in predicting temperature, a key indicator of global warming levels around the world, in the figure below. Given a temperature grid at a particular resolution **R** at a time stamp **T**, temporal forecasting focuses on predicting the temperature at a future time stamp **T+H** where **H** is the prediction horizon whereas the climate downscaling focuses on predicting the temperature at the same timestep T but at a higher resolution **(R' > R)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-YYAVH-V6ds"
   },
   "source": [
    "<a name=\"contributions\"></a>\n",
    "# Contributions\n",
    "\n",
    "Through introductory presentations and colab notebooks, we aim to expose the participants to: \n",
    "\n",
    "(a) **APIs** for accessing and navigating popular repositories that host global climate data, such as the Copernicus data store. \\\n",
    "(b) Identifying **relevant datasets** and providing functionality to preprocess them. \\\n",
    "(c) **Algorithms** for training machine learning models. \\\n",
    "(d) **Metrics** for evaluating model performance. \\\n",
    "(e) **Visualization** tools for both the dataset and predicted outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSRCNgYzUwaf"
   },
   "source": [
    "<a name=\"software-requirements\"></a>\n",
    "# Software Requirements\n",
    "This notebook requires the following libraries:\n",
    "*   climate_learn (pip)\n",
    "\n",
    "`climate_learn` contains the source files used for modeling climate extremes.\n",
    "\n",
    "The package is written using `PyTorch` machine learning library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnC-ra6sayXj"
   },
   "source": [
    "<a name=\"data-source\"></a>\n",
    "# Data Source\n",
    "\n",
    "[1] provides a comprehensive list of typical datasets and benchmarks for weather prediction. \n",
    "\n",
    "Among the list of popular datasets, we focus this tutorial on the **ERA5** dataset that provides an hourly estimate of a large number of atmospheric, land and oceanic climate variables. \n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://cds.climate.copernicus.eu/sites/default/files/cdsapp/copernicus-logo.png\" width=300></center>\n",
    "<br>\n",
    "\n",
    "ERA5 consists of the climate variable data from 1959 to present. The ERA5 dataset is freely-accessible to the community on the Climate Data Store [(CDS)](https://cds.climate.copernicus.eu/#!/search?text=ERA5&type=dataset) hosted by the European Union's Earth observation programme, [Copernicus](https://www.copernicus.eu/en/about-copernicus).  The users can visit [https://cds.climate.copernicus.eu/api-how-to](https://cds.climate.copernicus.eu/api-how-to#install-the-cds-api-key) to install the CDS API key required to access their data. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The authors from [2] (WeatherBench dataset) downloaded the ERA5 data from CDS API for a subset of climate variable and preprocessed it at multiple resolutions. The data is freely accessible to anyone at [https://dataserv.ub.tum.de/index.php/s/m1524895?path=%2F](https://dataserv.ub.tum.de/index.php/s/m1524895?path=%2F).\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://github.com/ai4environment/WeatherBench/raw/master/figures/logo_text_left.png?raw=true\" width=500></center>\n",
    "<br>\n",
    "\n",
    "_Our package provides the users with a way to download data from ERA5 as well as WeatherBench depending upon their application._\n",
    "\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1] Ren X, Li X, Ren K, Song J, Xu Z, Deng K, Wang X. Deep learning-based weather prediction: a survey. Big Data Research. 2021 Feb 15;23:100178 [(Paper)](https://www.sciencedirect.com/science/article/pii/S2214579620300460). \\\n",
    "[2] Rasp S, Dueben PD, Scher S, Weyn JA, Mouatadid S, Thuerey N. WeatherBench: a benchmark data set for data‐driven weather forecasting. Journal of Advances in Modeling Earth Systems. 2020 Nov;12(11):e2020MS002203 [(Paper)](https://arxiv.org/abs/2002.00469)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99jkSa_KmrDH"
   },
   "source": [
    "<a name=\"temporal-forecasting\"></a>\n",
    "# Temporal Forecasting\n",
    "\n",
    "\n",
    "A precise and reliable weather forecasting is of great importance in various aspect of society including precipitation forecasts are essential for agricultural needs, wind speed and solar power forecasts for energy generation.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1_tsaaogqzkYVV0jdawnO9GCJTToW_FCi\" height=300></center>\n",
    "\n",
    "The forecasting task can be categorized into (a) **nowcasting** (timescale of a few hours), (b) weather-scale prediction (typically 1day - 1week), (c) **seasonal** prediction (typically months) and (d) **multi-year or decadal** (timescale of multiple years).\n",
    "\n",
    "In this tutorial, we shall focus on the **medium-range** weather-scale  prediction of the climate variables i.e., typically 3-5 days in the future. This colab notebook demonstrates the temporal forecasting of the *Temperature* variable at *2m* height above the earth's surface. This variable serves as a good indicator of future temperatures on the Earth's surface for the forecasters. \n",
    "\n",
    "We shall further use the 2m temperature data at 5.625 degree resolution that divides the Earth's surface into a latitude x longitude grid of 32 x 64.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "\n",
    "**References:**\n",
    "1. Rasp S, Dueben PD, Scher S, Weyn JA, Mouatadid S, Thuerey N. WeatherBench: a benchmark data set for data‐driven weather forecasting. Journal of Advances in Modeling Earth Systems. 2020 Nov;12(11):e2020MS002203 [(Paper)](https://arxiv.org/abs/2002.00469).\n",
    "2. Civitarese DS, Szwarcman D, Zadrozny B, Watson C. Extreme Precipitation Seasonal Forecast Using a Transformer Neural Network. arXiv preprint arXiv:2107.06846. 2021 Jul 14. [(Paper)](https://arxiv.org/abs/2107.06846)\n",
    "3. Sønderby CK, Espeholt L, Heek J, Dehghani M, Oliver A, Salimans T, Agrawal S, Hickey J, Kalchbrenner N. Metnet: A neural weather model for precipitation forecasting. arXiv preprint arXiv:2003.12140. 2020 Mar 24. [(Paper)](https://arxiv.org/pdf/2003.12140.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTzr2Liw-SEv"
   },
   "outputs": [],
   "source": [
    "from climate_learn.data import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmQG73ZpQNHP"
   },
   "outputs": [],
   "source": [
    "# Download data from weatherbench (~2-3 minutes)\n",
    "download(root = \".\", source = \"weatherbench\", variable = \"2m_temperature\", dataset = \"era5\", resolution = \"5.625\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zb1e8ndtXiV-"
   },
   "source": [
    "ERA5 dataset directory structure from the Weatherbench source.\n",
    "\n",
    "```\n",
    "|-- 5.625deg\n",
    "|   |-- 2m_temperature\n",
    "|       |-- 2m_temperature_1979_5.625deg.nc\n",
    "|       |-- 2m_temperature_1980_5.625deg.nc\n",
    "|       |-- ...\n",
    "|       |-- 2m_temperature_2018_5.625deg.nc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSt6h_Q-oqjK"
   },
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8OF_Oj60a2g"
   },
   "source": [
    "The data is stored in the [NetCDF](https://en.wikipedia.org/wiki/NetCDF) files with _.nc_ extension. One of the distinct features of this format is the **named** specification to the coordinates and the data variables. \n",
    "\n",
    "As shown below, we first merge all the yearly NetCDF files, and display the structure of the format. xarray library is used to read the NetCDF files. It allows the users to manipulate data based on more informative labels instead of integer location. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97hHL2Z7-Z86"
   },
   "outputs": [],
   "source": [
    "from climate_learn.utils.data import load_dataset, view\n",
    "\n",
    "dataset = load_dataset(\"./data/weatherbench/era5/5.625/2m_temperature\")\n",
    "view(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XM3rITW9Y3-"
   },
   "source": [
    "## Data Conversion\n",
    "We further convert the *NetCDF* files to *PyTorch* Dataloaders.\n",
    "\n",
    "**Pros**: We can use the dataloaders for training and evaluating neural networks.\\\n",
    "**Cons**: We loose useful meta information (such as 'time', 'location') during conversion as dataloaders only allow for integer location based treatment. \n",
    "\n",
    "We store the useful information about\n",
    " the data ('lat', 'long') of the regions as _data members_ of our dataloaders.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIymAwxkd9O-"
   },
   "source": [
    "### Example\n",
    "\n",
    "- In the given example, we train our model from the data from 1979 (start train year) -2015, validate it from the data from 2015 (start validation year) - 2016, and test it on data from 2017 (start test year) - 2018 (end test year).\n",
    "- We split the data this way as it allows us to test the temporal generalization of our models. Additionally, it replicates more practical setting where the model that is trained on the historical data is used for forecasting in the future.\n",
    "- pred_range = Days(3) indicates that we are going to predict 3 days in the future.\n",
    "- subsample = Hours(6) indicates that we shall use the data at every 6 hours from 24 hours of the data from a single day i.e., rather than using data at 12am, 1am, 2am...11pm from a single day, we shall use the data at 12am, 6am, 12pm, 6pm. As climate variables are highly correlated within a window of a few hours, subsampling helps in removing those redundancies during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EK2UD49hQ3om"
   },
   "outputs": [],
   "source": [
    "from climate_learn.utils.datetime import Year, Days, Hours\n",
    "from climate_learn.data.climate_dataset.args import ERA5Args\n",
    "from climate_learn.data.tasks.args import ForecastingArgs\n",
    "from climate_learn.data import DataModuleArgs, DataModule\n",
    "\n",
    "data_args = ERA5Args(\n",
    "    root_dir = \"./data/weatherbench/era5/5.625/\",\n",
    "    variables = [\"2m_temperature\"],\n",
    "    years = range(1979, 2015),\n",
    ")\n",
    "\n",
    "forecasting_args = ForecastingArgs(\n",
    "    dataset_args = data_args,\n",
    "    in_vars = [\"2m_temperature\"],\n",
    "    out_vars = [\"2m_temperature\"],\n",
    "    pred_range = 3*24,\n",
    "    subsample = 6,\n",
    ")\n",
    "\n",
    "data_module_args = DataModuleArgs(\n",
    "    task_args = forecasting_args,\n",
    "    train_start_year = 1979,\n",
    "    val_start_year = 2015,\n",
    "    test_start_year = 2017,\n",
    "    end_year = 2018,\n",
    ")\n",
    "\n",
    "data_module = DataModule(\n",
    "    data_module_args = data_module_args,\n",
    "    batch_size = 128,\n",
    "    num_workers = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1co6tQpiAoYO"
   },
   "source": [
    "## Neural Networks Architectures\n",
    "\n",
    "We consider three deep neural network architectures for in this tutorial.\n",
    "\n",
    "1. Convolutional Neural Networks (CNN)\n",
    "<center><img src=\"https://viso.ai/wp-content/uploads/2021/03/cnn-convolutional-neural-networks-1060x362.jpg\" width=400></center>\n",
    "\n",
    "CNN is a widely used neural network architecture for visual recognition. A CNN is a constrainted version of a regular neural network that takes advantage of the prior knowledge that the input is an image. The architecture can be flexibly adapted to various image resolutions. \n",
    "\n",
    "For our use case, the latitude-longitude gridded data is treated as an image composed of climate variables in its channel instead of the Red-Green-Blue (RGB) information.\n",
    "\n",
    "Resource(s): https://cs231n.github.io/convolutional-networks/ \n",
    "\n",
    "Among the long list of CNN variants availablely, we provide support for its popular variants. \n",
    "\n",
    "\n",
    "Variants of CNN architecture: \\\n",
    "\n",
    "a. **ResNet**\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/875/1*WpX_8eCeTsEcCs8vdXtUCw.png\" width=400></center>\n",
    "\n",
    "ResNets have been used to achieve SOTA weather forecasting using neural networks for temperature and geopotential in [1]. \n",
    "\n",
    "Paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "b. **U-Net**\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/875/1*f7YOaE4TWubwaFF7Z1fzNw.png\" width=400></center>\n",
    "\n",
    "The basic building blocks of the U-Net architecture involve downsampling as well as upsampling convolutions. The downsampling blocks project the input from higher dimension to a lower dimension, and upsampling blocks project the low dimension latent space to the higher dimension input space. After gaining popularity in the Biomedical domain, our package allows the users to benchmark U-Net in the Climate modeling space too.\n",
    "\n",
    "\n",
    "Paper: [U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "](https://arxiv.org/abs/1505.04597) \n",
    "\n",
    "\n",
    "\n",
    "2. Vision Transformers\n",
    "\n",
    "Vision transformers are the latest contemporary to CNN variants for visual recognition. We relegate the audience to the related paper for its architectural details.\n",
    "\n",
    "<center><img src=\"https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png\" width=400></center>\n",
    "\n",
    "Vision Transformers have gained immense popularity in the Vision community, and its usefulness to learn representations of climate variables is still under-explored. [2] used Transformers for short-range temperature forecasting.\n",
    "We believe that our ViT implementation shall allow the users to benchmark ViT on climate modeling tasks.\n",
    "\n",
    "Paper: <a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] Rasp S, Thuerey N. Data‐driven medium‐range weather prediction with a resnet pretrained on climate simulations: A new model for weatherbench. Journal of Advances in Modeling Earth Systems. 2021 Feb;13(2):e2020MS002405.\\\n",
    "[2] Bilgin O, Mąka P, Vergutz T, Mehrkanoon S. TENT: Tensorized encoder transformer for temperature forecasting. arXiv preprint arXiv:2106.14742. 2021 Jun 28.\n",
    "\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "In this tutorial, we shall demonstrate the training of a resnet from scratch. It is important to note that the choice of model architecture and hyperparameters are for demonstration purposes only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWXsiZ5freTG"
   },
   "source": [
    "## Model initialization \n",
    "\n",
    "The hyperparameters and ResNet architecture chosen allow for a model that forecasts with 85.7% test accuracy, while still training within a reasonable amount of time for the sake of the tutorial (by nature of being a smaller model). We leave it to the user to perform a more exhaustive search of hyperparameter values for training models that perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paTI33tP5R4H"
   },
   "outputs": [],
   "source": [
    "from climate_learn.models import load_model\n",
    "\n",
    "model_kwargs = {\n",
    "    \"in_channels\": len(data_module.hparams.data_module_args.train_task_args.in_vars),\n",
    "    \"out_channels\": len(data_module.hparams.data_module_args.train_task_args.out_vars),\n",
    "    \"n_blocks\": 4\n",
    "}\n",
    "\n",
    "optim_kwargs = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"warmup_epochs\": 1,\n",
    "    \"max_epochs\": 5,\n",
    "}\n",
    "\n",
    "model_module = load_model(name = \"resnet\", task = \"forecasting\", model_kwargs = model_kwargs, optim_kwargs = optim_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BalFce--51Rh"
   },
   "outputs": [],
   "source": [
    "from climate_learn.models import set_climatology\n",
    "set_climatology(model_module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLxhgSOctxo4"
   },
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "The training objective ensures that the machine learning model makes accurate forecasts over the gridded data. We employ latitude weighted RMSE given by:\n",
    "\n",
    "<br>\n",
    "$RMSE = \\frac{1}{N_{forecasts}}\\sum_{i}^{N_{forecasts}}\\sqrt{\\frac{1}{N_{lat}N_{lon}}\\sum_{j}^{N_{lat}}\\sum_{k}^{N_{lon}}L(j)(f_{i,j,k}-t_{i,j,k})^{2}} \\tag{1}$ \n",
    "<br>\n",
    "\n",
    "where $f$ is the model forecast and $t$ is the ERA5 truth. $L(j)$ is the latitude weighing factor at the $j^{th}$ latitude index:\n",
    "\n",
    "<br>\n",
    "$L(j) = \\frac{cos(lat(j))}{\\frac{1}{N_{lat}}\\sum_{j}^{N_{lat}}cos(lat(j))} \\tag{2}$\n",
    "<br>\n",
    "\n",
    "(Optional) If you want to monitor training and validation curves of the model using [Weights and Biases](https://docs.wandb.ai/), uncomment the lines in the following code block and login to your Wandb account (only once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqh0TzP4TebP"
   },
   "outputs": [],
   "source": [
    "from climate_learn.training import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    seed = 0,\n",
    "    accelerator = \"gpu\",\n",
    "    precision = 16,\n",
    "    max_epochs = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwZ49aNGhhUt"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model_module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCR5yoIAUBTa"
   },
   "source": [
    "## Evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlnsuXsbwvPo"
   },
   "source": [
    "Once our prediction model is trained, we want to be able to evaluate it against the ground truth labels for data samples in the test set. \n",
    "\n",
    "In addition to the Latitude weighted RMSE (Eq. 1), we shall look at the Anomaly Correlation Coefficient (ACC) which is defined as:\n",
    "\n",
    "<br>\n",
    "$ACC = \\frac{\\sum_{i,j,k}L(j)f'_{i,j,k}t'_{i,j,k}}{\\sqrt{\\sum_{i,j,k}L(j)f'^{2}_{i,j,k}L(j)t'^{2}_{i,j,k}}} \\tag{3}$\n",
    "<br>\n",
    "\n",
    "where $'$ denotes the difference to the climatology. We define climatology as:\n",
    "\n",
    "<br>\n",
    "$climatology_{j,k} = \\frac{1}{N_{time}}\\sum{t_{j,k}}\\tag{4}$\n",
    "<br>\n",
    "\n",
    "For the RMSE metric, we compare the deep learning model with a climatological forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvdXm85fa9KD"
   },
   "outputs": [],
   "source": [
    "trainer.test(model_module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWigLES4s22j"
   },
   "source": [
    "The model's prediction has a strong correlation with the ground truth, which is indicated by a high ACC value. Compared to a climatological forecast, the deep learning model achieves a much smaller RMSE error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcPCvx8AbPFZ"
   },
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRQLNyO_yPhn"
   },
   "source": [
    "We visualize the **bias**, given by the difference in the predicted and the ground truth values, to better analyze our learned model.\n",
    "\n",
    "Visualization is done on the test set. We can either specify exact time for the initial condition, or randomly sample from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-AM0-CPbTNl"
   },
   "outputs": [],
   "source": [
    "from climate_learn.utils import visualize\n",
    "\n",
    "# if samples = 2, we randomly pick 2 initial conditions in the test set\n",
    "visualize(model_module, data_module, samples = [\"2017-06-01:12\", \"2017-08-01:18\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy3WrJEOsEXZ"
   },
   "source": [
    "The visualization shows that the model makes reasonably accurate predictions in most parts of the globe, and the prediction well correlates with the ground truth. The model seems to make large errors near the two poles, where the temperature is more unpredictable. We can improve the accuracy of the model by either including more input variables, using a more expressive model, or training for longer. For the sake of this tutorial, we use a small model and only train for a short time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6HYhl551E8_"
   },
   "source": [
    "In addition to visualizing the bias the model makes for each individual data point, we can also visualize the mean bias across the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7Qjfu-H1VEd"
   },
   "outputs": [],
   "source": [
    "from climate_learn.utils import visualize_mean_bias\n",
    "visualize_mean_bias(model_module.cuda(), data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srfsF01OLV-C"
   },
   "source": [
    "<a name=\"spatial-downscaling\"></a>\n",
    "# Spatial Downscaling\n",
    "\n",
    "\n",
    "General Circulation Models (GCMs) provide us with the future projections of climate scenarios. These raw estimates have to be downscaled at the desired resolution for actionable guidance.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=11i2CIRxlVRqOHIgZRABwF05Qf5KeqVwc\" height=300></center>\n",
    "\n",
    "In practice, statistical spatial downscaling can be used to make predictions about a climate variable (a) over the latitude-longitude grid of **higher** resolution than the input grid and (b) on specific sites at the target locations. For example, we can predict the temperature at a specific station in Germany based on the gridded temperature data over the whole country.\n",
    "\n",
    "Major class of statistical downscaling models include Perfect Prognosis (PP) [1] that aims at learning a transfer function $$\\hat{y} = f(x, Z)$$ where $y$ is the true value at location $x$ and $Z$ are the set of model predictors for the climate model. The various PP models differ in their realization of the transfer function $f$. Related works in [2] provides deeper details into the previous works. In [2,3], the authors use CNNs as the transfer function, broadly due to its inherent inductive bias towards handling Vision data. The ability of Deep CNNs to perform super-resolution is a well-explored field of study [4].\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**References:**\n",
    "1. Maraun D, Wetterhall F, Ireson AM, Chandler RE, Kendon EJ, Widmann M, Brienen S, Rust HW, Sauter T, Themeßl M, Venema VK. Precipitation downscaling under climate change: Recent developments to bridge the gap between dynamical models and the end user. Reviews of geophysics. 2010 Sep;48(3).\n",
    "2. Vaughan A, Tebbutt W, Hosking JS, Turner RE. Convolutional conditional neural processes for local climate downscaling. arXiv preprint arXiv:2101.07950. 2021 Jan 20.\n",
    "3. Baño-Medina J, Manzanas R, Gutiérrez JM. Configuration and intercomparison of deep learning neural models for statistical downscaling. Geoscientific Model Development. 2020 Apr 28;13(4):2109-24.\n",
    "4. Yamanaka J, Kuwashima S, Kurita T. Fast and accurate image super resolution by deep CNN with skip connection and network in network. InInternational Conference on Neural Information Processing 2017 Nov 14 (pp. 217-225). Springer, Cham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwdjVOCoJP9t"
   },
   "source": [
    "- In this tutorial, we shall focus on mapping the coarse resolution data for a variable to a finer resolution at a given time stamp. Specifically, we shall continue with focusing on the _Temperature at 2m_ climate variable using a ResNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zAegEGoV9zy"
   },
   "source": [
    "## Data Download\n",
    "\n",
    "To perfrom climate downscaling, we need to have data for the temperature at 2m at different resolutions. In addition to the 5.625deg dataset we downloaded above, here we download the 2.8125deg dataset, which divides the Earth's surface into a latitude x longitude grid of 64 x 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3tRve6-h0sI"
   },
   "outputs": [],
   "source": [
    "from climate_learn.data import download\n",
    "\n",
    "# Download data from weatherbench (~4-6 minutes)\n",
    "download(root = \".\", source = \"weatherbench\", variable = \"2m_temperature\", dataset = \"era5\", resolution = \"2.8125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsR8lhdjXejR"
   },
   "source": [
    "## Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T9N7cL4oFKm"
   },
   "outputs": [],
   "source": [
    "from climate_learn.utils.datetime import Year, Days, Hours\n",
    "from climate_learn.data.climate_dataset.args import ERA5Args\n",
    "from climate_learn.data.tasks.args import DownscalingArgs\n",
    "from climate_learn.data import DataModuleArgs, DataModule\n",
    "\n",
    "lowres_data_args = ERA5Args(\n",
    "    root_dir = \"./data/weatherbench/era5/5.625\",\n",
    "    variables = [\"2m_temperature\"],\n",
    "    years = range(1979, 2015),\n",
    ")\n",
    "\n",
    "highres_data_args = ERA5Args(\n",
    "    root_dir = \"./data/weatherbench/era5/2.8125\",\n",
    "    variables = [\"2m_temperature\"],\n",
    "    years = range(1979, 2015),\n",
    ")\n",
    "\n",
    "downscaling_args = DownscalingArgs(\n",
    "    dataset_args = data_args,\n",
    "    highres_dataset_args = highres_data_args,\n",
    "    in_vars = [\"2m_temperature\"],\n",
    "    out_vars = [\"2m_temperature\"],\n",
    "    subsample = 6,\n",
    ")\n",
    "\n",
    "data_module_args = DataModuleArgs(\n",
    "    task_args = downscaling_args,\n",
    "    train_start_year = 1979,\n",
    "    val_start_year = 2015,\n",
    "    test_start_year = 2017,\n",
    "    end_year = 2018,\n",
    ")\n",
    "\n",
    "data_module = DataModule(\n",
    "    data_module_args = data_module_args,\n",
    "    batch_size = 128,\n",
    "    num_workers = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sunkptw4g_Rc"
   },
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqxpa1znpX5n"
   },
   "outputs": [],
   "source": [
    "from climate_learn.models import load_model\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model_kwargs = {\n",
    "    \"in_channels\": len(data_module.hparams.data_module_args.train_task_args.in_vars),\n",
    "    \"out_channels\": len(data_module.hparams.data_module_args.train_task_args.out_vars),\n",
    "    \"upsampling\": data_module.train_dataset.downscale_ratio,\n",
    "    \"n_blocks\": 4,\n",
    "}\n",
    "\n",
    "optim_kwargs = {\n",
    "    \"optimizer\": AdamW,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"warmup_epochs\": 1,\n",
    "    \"max_epochs\": 5,\n",
    "}\n",
    "\n",
    "model_module = load_model(name = \"resnet\", task = \"downscaling\", model_kwargs = model_kwargs, optim_kwargs = optim_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHytyooUpc9-"
   },
   "outputs": [],
   "source": [
    "# latitude long info, \n",
    "from climate_learn.models import set_climatology\n",
    "set_climatology(model_module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1XU1EIvXhvb"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioW1bg4QpZ30"
   },
   "outputs": [],
   "source": [
    "from climate_learn.training import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    seed = 0,\n",
    "    accelerator = \"gpu\",\n",
    "    precision = 16,\n",
    "    max_epochs = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6K0-H3ppiRY"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model_module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7JdGELMXpIw"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kELMwe8lpm1e"
   },
   "outputs": [],
   "source": [
    "trainer.test(model_module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DQou7zmXq7u"
   },
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SL2v8Nn973sL"
   },
   "outputs": [],
   "source": [
    "visualize(model_module, data_module, samples = [\"2017-06-01:12\", \"2017-08-01:18\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2wWs9DDxb1I"
   },
   "source": [
    "## Potential Applications of the Tutorial\n",
    "\n",
    "\n",
    "1. Our package with this tutorial makes it easy for the practitioners to focus on the machine learning model design and evaluate their models against standard benchmarks such as Weatherbench ([Leaderboard](https://github.com/pangeo-data/WeatherBench)) using our evaluation support. \n",
    "2. The climate experts can load and train their customized machine learning models with various hyperparameters. For instance, they can study the effect of varying the capacity of machine learning models on its predictions.\n",
    "3. To best of our knowledge, this is the only package that provides support for temporal forecasting and spatial downscaling under the hood. It helps the forecasting community to benefit from the improvements in modeling downscaling and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9VLYXAkxfAH"
   },
   "source": [
    "## Limitations\n",
    "\n",
    "\n",
    "1. Due to the noise involved in capturing climate variables, climate modeling is particularly a challenging task. Different datasets such as ERA5, CMIP6 may contain different values for a particular climate variable. \n",
    "2. This tutorial does not cover the aspect of uncertainity in the model predictions. We shall add more capabilities to our models in the package for climate modelers to quantify the error in the predicted output.\n",
    "3. There is a massive amount of climate data available (~10 TBs), our tutorial does not cover aspects of how to deal with the data at that scale which might be important for capturing more sophisticated climate phenomenon."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "cl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b35d5811d64db97cad819926e9e0ba09b354a75e2ee95b259c11201fc783944"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
